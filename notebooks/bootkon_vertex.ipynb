{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad2b88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 4: Machine Learning with Vertex AI\n",
    "\n",
    "Author: \n",
    "* Fabian Hirschmann <<fhirschmann@google.com>>\n",
    "\n",
    "Welcome back üëãüòç. During this lab, you will train a machine learning model on the data set you already know. We will deploy it to Vertex AI and finally construct a machine learning pipeline to perform the training process automatically.\n",
    "\n",
    "We will do it in three different maturity levels:\n",
    "\n",
    "1. Deploying locally trained models to Vertex AI using prebuilt containers\n",
    "2. Train and deploy model on Vertex AI using custom containers\n",
    "3. Use Vertex AI pipeline to train and deploy the model\n",
    "\n",
    "In this Jupyter Notebook, you can press `Shift + Return` to execute the current code junk and jump to the next one.\n",
    "\n",
    "## Step 0: Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07b179a-c674-4f9e-83d8-245c643400b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet \\\n",
    "    google-cloud-aiplatform==1.72.0 \\\n",
    "    google-cloud-bigquery \\\n",
    "    google-cloud-bigquery \\\n",
    "    google-cloud-logging \\\n",
    "    google-cloud-pipeline-components==2.18.0 \\\n",
    "    fastavro \\\n",
    "    avro \\\n",
    "    pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be78b5f-9903-4ff8-af5c-d3c53f303e31",
   "metadata": {},
   "source": [
    "Once the command above has finished, <font color=red>please restart your kernel from the menu (Kernel -> Restart Kernel) and continue with step 1.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b2872-2386-4f78-93aa-ebb435721a96",
   "metadata": {},
   "source": [
    "## Step 1: Import Dependencies and Set Environment Variables\n",
    "\n",
    "Before we begin, let's import the necessary Python libraries and set a few environment variables for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf0314c-770c-41fe-be57-f41eb17bf48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from sklearn.metrics import roc_curve, auc as auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "import google.cloud.logging\n",
    "google.cloud.logging.Client().setup_logging(log_level=logging.WARNING)\n",
    "\n",
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "BQ_DATASET = \"ml_datasets\"\n",
    "BQ_TABLE = \"ulb_fraud_detection_dataproc\"\n",
    "BQ_SOURCE = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
    "PIPELINE_ROOT = f\"gs://{PROJECT_ID}-bucket/pipelines\"\n",
    "TRAIN_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bootkon/bootkon-train:latest\"\n",
    "PREDICT_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bootkon/bootkon-predict:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9ce43-7912-4fcc-98ee-e30ae961e43e",
   "metadata": {},
   "source": [
    "## Step 2: Create dataset for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8cb35-8c90-4378-a592-fd28ffa6d1a5",
   "metadata": {},
   "source": [
    "We initialize the AI Platform and BigQuery client to interact with Google Cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ea1163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"{PROJECT_ID}-bucket\")\n",
    "bq = bigquery.Client(project=PROJECT_ID, location=\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c15d1c-82ed-460c-80d3-c08c5bee1432",
   "metadata": {},
   "source": [
    "The BigQuery table we'll be working with is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e001be2f-5701-497d-b4a4-9eb8d2202fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astute-ace-336608.ml_datasets.ulb_fraud_detection_dataproc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BQ_SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46092b6-2908-4bf6-b45f-53697f2b17c6",
   "metadata": {},
   "source": [
    "We execute a query to fetch the dataset from BigQuery and store it in a Pandas DataFrame. We don't need the `Feedback` column for machine learning -- so we will delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07455370-2100-469e-9a87-09963c593437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = bq.query(f\"SELECT * FROM `{BQ_SOURCE}`\").to_dataframe()\n",
    "data.drop(\"Feedback\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3aaa1-4083-4e35-8c70-6ecbf41fac76",
   "metadata": {},
   "source": [
    "Let's have a look at the data set in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590bc934-a254-4ee3-9dc4-1a21cf57648d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67573.0</td>\n",
       "      <td>1.247292</td>\n",
       "      <td>-1.214023</td>\n",
       "      <td>1.717765</td>\n",
       "      <td>-0.129889</td>\n",
       "      <td>-2.118590</td>\n",
       "      <td>0.358109</td>\n",
       "      <td>-1.763690</td>\n",
       "      <td>0.452637</td>\n",
       "      <td>0.986597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164138</td>\n",
       "      <td>0.776612</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.428537</td>\n",
       "      <td>0.297247</td>\n",
       "      <td>-0.029560</td>\n",
       "      <td>0.090474</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127999.0</td>\n",
       "      <td>-1.534523</td>\n",
       "      <td>1.122946</td>\n",
       "      <td>-3.437084</td>\n",
       "      <td>-0.797825</td>\n",
       "      <td>1.015405</td>\n",
       "      <td>-1.250023</td>\n",
       "      <td>0.585146</td>\n",
       "      <td>0.872816</td>\n",
       "      <td>-0.892797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516332</td>\n",
       "      <td>1.529644</td>\n",
       "      <td>0.246227</td>\n",
       "      <td>0.313690</td>\n",
       "      <td>-0.999467</td>\n",
       "      <td>0.660153</td>\n",
       "      <td>0.258338</td>\n",
       "      <td>-0.068222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145149.0</td>\n",
       "      <td>2.156547</td>\n",
       "      <td>0.100973</td>\n",
       "      <td>-2.700733</td>\n",
       "      <td>-0.048208</td>\n",
       "      <td>1.143939</td>\n",
       "      <td>-0.896822</td>\n",
       "      <td>0.817691</td>\n",
       "      <td>-0.407689</td>\n",
       "      <td>-0.310319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>-0.195982</td>\n",
       "      <td>0.389867</td>\n",
       "      <td>0.657228</td>\n",
       "      <td>0.993540</td>\n",
       "      <td>-0.157356</td>\n",
       "      <td>-0.106062</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153187.0</td>\n",
       "      <td>-1.070276</td>\n",
       "      <td>0.750548</td>\n",
       "      <td>-0.432043</td>\n",
       "      <td>0.795662</td>\n",
       "      <td>1.894683</td>\n",
       "      <td>-0.913714</td>\n",
       "      <td>1.370461</td>\n",
       "      <td>-0.728018</td>\n",
       "      <td>-0.391775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084740</td>\n",
       "      <td>0.626613</td>\n",
       "      <td>-0.215756</td>\n",
       "      <td>0.581448</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>-0.465960</td>\n",
       "      <td>-0.551073</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152869.0</td>\n",
       "      <td>2.136909</td>\n",
       "      <td>0.088646</td>\n",
       "      <td>-2.490914</td>\n",
       "      <td>0.098321</td>\n",
       "      <td>0.789008</td>\n",
       "      <td>-1.399582</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>-0.492912</td>\n",
       "      <td>-0.254999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278034</td>\n",
       "      <td>0.934892</td>\n",
       "      <td>-0.211839</td>\n",
       "      <td>-0.234266</td>\n",
       "      <td>0.609699</td>\n",
       "      <td>1.020898</td>\n",
       "      <td>-0.154427</td>\n",
       "      <td>-0.112532</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>63644.0</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>-0.050017</td>\n",
       "      <td>0.745618</td>\n",
       "      <td>0.992303</td>\n",
       "      <td>-0.024952</td>\n",
       "      <td>1.071646</td>\n",
       "      <td>-0.465425</td>\n",
       "      <td>0.532930</td>\n",
       "      <td>0.318431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054716</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>0.281037</td>\n",
       "      <td>-0.703291</td>\n",
       "      <td>-0.102724</td>\n",
       "      <td>-0.512912</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>82872.0</td>\n",
       "      <td>1.562782</td>\n",
       "      <td>-1.229601</td>\n",
       "      <td>-1.158630</td>\n",
       "      <td>-2.496174</td>\n",
       "      <td>0.974909</td>\n",
       "      <td>3.237633</td>\n",
       "      <td>-1.478926</td>\n",
       "      <td>0.745097</td>\n",
       "      <td>-1.985665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263261</td>\n",
       "      <td>-0.527888</td>\n",
       "      <td>-0.006884</td>\n",
       "      <td>0.982099</td>\n",
       "      <td>0.546208</td>\n",
       "      <td>-0.191959</td>\n",
       "      <td>0.031388</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>143865.0</td>\n",
       "      <td>2.277261</td>\n",
       "      <td>-1.618792</td>\n",
       "      <td>-2.363003</td>\n",
       "      <td>-2.596243</td>\n",
       "      <td>1.166023</td>\n",
       "      <td>3.433015</td>\n",
       "      <td>-1.622363</td>\n",
       "      <td>0.798377</td>\n",
       "      <td>-1.403863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166966</td>\n",
       "      <td>-0.075929</td>\n",
       "      <td>0.263730</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>-0.130091</td>\n",
       "      <td>-0.140835</td>\n",
       "      <td>0.027717</td>\n",
       "      <td>-0.056861</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>36081.0</td>\n",
       "      <td>1.306817</td>\n",
       "      <td>-0.541898</td>\n",
       "      <td>0.166026</td>\n",
       "      <td>-0.632248</td>\n",
       "      <td>-0.698127</td>\n",
       "      <td>-0.418271</td>\n",
       "      <td>-0.424042</td>\n",
       "      <td>-0.036891</td>\n",
       "      <td>-1.062217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250819</td>\n",
       "      <td>0.764577</td>\n",
       "      <td>-0.132648</td>\n",
       "      <td>0.281172</td>\n",
       "      <td>0.660827</td>\n",
       "      <td>-0.066862</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>-0.005665</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>55262.0</td>\n",
       "      <td>-1.732064</td>\n",
       "      <td>0.924056</td>\n",
       "      <td>1.082680</td>\n",
       "      <td>0.567256</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>-0.172493</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.166905</td>\n",
       "      <td>-0.221053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046174</td>\n",
       "      <td>0.273960</td>\n",
       "      <td>0.245251</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>-0.043749</td>\n",
       "      <td>-0.532553</td>\n",
       "      <td>-0.863063</td>\n",
       "      <td>-0.369561</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0        67573.0  1.247292 -1.214023  1.717765 -0.129889 -2.118590  0.358109   \n",
       "1       127999.0 -1.534523  1.122946 -3.437084 -0.797825  1.015405 -1.250023   \n",
       "2       145149.0  2.156547  0.100973 -2.700733 -0.048208  1.143939 -0.896822   \n",
       "3       153187.0 -1.070276  0.750548 -0.432043  0.795662  1.894683 -0.913714   \n",
       "4       152869.0  2.136909  0.088646 -2.490914  0.098321  0.789008 -1.399582   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802   63644.0  0.971875 -0.050017  0.745618  0.992303 -0.024952  1.071646   \n",
       "284803   82872.0  1.562782 -1.229601 -1.158630 -2.496174  0.974909  3.237633   \n",
       "284804  143865.0  2.277261 -1.618792 -2.363003 -2.596243  1.166023  3.433015   \n",
       "284805   36081.0  1.306817 -0.541898  0.166026 -0.632248 -0.698127 -0.418271   \n",
       "284806   55262.0 -1.732064  0.924056  1.082680  0.567256 -0.007326 -0.172493   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0      -1.763690  0.452637  0.986597  ...  0.164138  0.776612  0.004733   \n",
       "1       0.585146  0.872816 -0.892797  ...  0.516332  1.529644  0.246227   \n",
       "2       0.817691 -0.407689 -0.310319  ...  0.267644  0.882000 -0.195982   \n",
       "3       1.370461 -0.728018 -0.391775  ...  0.084740  0.626613 -0.215756   \n",
       "4       0.854902 -0.492912 -0.254999  ...  0.278034  0.934892 -0.211839   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284802 -0.465425  0.532930  0.318431  ... -0.054716 -0.102631  0.281037   \n",
       "284803 -1.478926  0.745097 -1.985665  ... -0.263261 -0.527888 -0.006884   \n",
       "284804 -1.622363  0.798377 -1.403863  ... -0.166966 -0.075929  0.263730   \n",
       "284805 -0.424042 -0.036891 -1.062217  ...  0.250819  0.764577 -0.132648   \n",
       "284806  0.323600  0.166905 -0.221053  ...  0.046174  0.273960  0.245251   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.428537  0.297247 -0.029560  0.090474  0.022572     2.0      0  \n",
       "1       0.313690 -0.999467  0.660153  0.258338 -0.068222     2.0      0  \n",
       "2       0.389867  0.657228  0.993540 -0.157356 -0.106062     2.0      0  \n",
       "3       0.581448  0.018814 -0.465960 -0.551073  0.009276     2.0      0  \n",
       "4      -0.234266  0.609699  1.020898 -0.154427 -0.112532     2.0      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "284802 -0.703291 -0.102724 -0.512912  0.098703  0.023487    20.0      0  \n",
       "284803  0.982099  0.546208 -0.191959  0.031388  0.015444    20.0      0  \n",
       "284804  0.687151 -0.130091 -0.140835  0.027717 -0.056861    20.0      0  \n",
       "284805  0.281172  0.660827 -0.066862  0.003071 -0.005665    20.0      0  \n",
       "284806  0.185157 -0.043749 -0.532553 -0.863063 -0.369561    20.0      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02afc5-90bb-479d-bd8e-40f0d0eb4c9b",
   "metadata": {},
   "source": [
    "We separate the target variable (`Class`), which we want to predict, from the features (all other columns). The `Class` column indicates whether a transaction is fraudulent (1) or legitimate (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5707b149-4f88-4375-ba9b-878781b57eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = data[\"Class\"].astype(int)\n",
    "data.drop(\"Class\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c56564-6a0a-4726-8d55-ed7f3730dfa2",
   "metadata": {},
   "source": [
    "Fraud detection datasets are typically highly imbalanced, meaning the majority of transactions are legitimate. We check the distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfefa617-6cd7-40f3-81ae-4aa1c2aa2ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d7aa-bf49-4767-a588-22ab47d78a14",
   "metadata": {},
   "source": [
    "We split our dataset into two parts:\n",
    "\n",
    "- Training set (80%): Used to train the machine learning model.\n",
    "- Testing set (20%): Used to evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec82ed34-ae5d-4102-a67d-b3f6956a435c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00319219-04c0-4f9c-a59e-eaa3809ef84b",
   "metadata": {},
   "source": [
    "Let's also save it to Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b804dd-9d24-46d4-8f23-cf2b1340dbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\", index=False)\n",
    "y_train.to_frame().to_csv(f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7627f-aacd-401f-b32b-d39f690c3172",
   "metadata": {},
   "source": [
    "## Step 3.1: Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9bdfa-a156-45db-b466-2a10e1539a78",
   "metadata": {},
   "source": [
    "We use a `RandomForestClassifier`, which is an ensemble learning method that creates multiple decision trees and aggregates their predictions. This helps improve accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f91fb92-47a9-4e93-8a68-35974c0fbca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:   55.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=8, verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7cd00-ded8-4eac-a4a4-6ed12abc0ce0",
   "metadata": {},
   "source": [
    "We calculate the accuracy of the model, which measures the proportion of correctly classified instances.\n",
    "\n",
    "For a highly imbalanced data set, the accuracy is often meaningless, because a simple classifier that always says ***not fraud*** will have an accuracy close to 1 already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35eb9ac3-0897-429d-b2cc-4e4d440cb648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995611109160493"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e458e-03a3-4d2a-9b97-2b4760ac7118",
   "metadata": {},
   "source": [
    "We compute the ROC AUC (Receiver Operating Characteristic - Area Under the Curve) score. This metric evaluates the model's ability to distinguish between classes. A score closer to 1 indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb3e080-b7ff-46c6-aff0-7da10ed4b76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9219180813470536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb1b11-aba7-46a4-89a7-0c020d34fef9",
   "metadata": {},
   "source": [
    "We save the trained model to a local file so we can deploy it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b1c581a-7f80-4582-9c56-bf3a3563001a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5826c-7da4-42f8-bcc6-dd5188acbf59",
   "metadata": {},
   "source": [
    "We upload the trained model to Vertex AI, where it can be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a9fe3-1f24-4fef-a870-d49f4fd99d37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.joblib gs://{PROJECT_ID}-bucket/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150706a1-c207-4051-94e2-b8acac8387e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3.2: Serve locally trained model on Vertex AI\n",
    "\n",
    "The Vertex AI Model Registry is a centralized repository in Google Cloud's Vertex AI platform where machine learning (ML) models are stored, managed, and versioned. It allows data scientists and ML engineers to track different model versions, store metadata, and deploy models seamlessly to Vertex AI endpoints for inference.\n",
    "\n",
    "Key features of the Model Registry include:\n",
    "\n",
    "* Model Versioning: Track multiple versions of a model.\n",
    "* Metadata Management: Store details such as model parameters, training data, and performance metrics.\n",
    "* Deployment & Serving: Deploy registered models to Vertex AI Endpoints, Batch Predictions, or export them for external use.\n",
    "* Model Governance: Manage access control, approval workflows, and lineage tracking.\n",
    "* Integration with Pipelines: Automate model registration via Vertex AI Pipelines.\n",
    "\n",
    "We can register the model we just trained in this notebook as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf07af9-6490-4a6e-9ff8-6304eeb5feff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/888342260584/locations/us-central1/models/3048036447706677248/operations/5520722546774769664\n",
      "Model created. Resource name: projects/888342260584/locations/us-central1/models/3048036447706677248@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/888342260584/locations/us-central1/models/3048036447706677248@1')\n"
     ]
    }
   ],
   "source": [
    "vertex_model_upload = aiplatform.Model.upload(\n",
    "    display_name=\"bootkon-upload-model\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest\",\n",
    "    artifact_uri=f\"gs://{PROJECT_ID}-bucket/model/\",\n",
    "    is_default_version=True,\n",
    "    version_aliases=[\"v1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc86b2-ddfb-4b7b-a129-e3127a6c7362",
   "metadata": {},
   "source": [
    "Once the model has been uploaded, navigate to the [`Model Registry` in Vertex AI](https://console.cloud.google.com/vertex-ai/models). Click on `bootkon-model`. Can you find your newly created model artifact? Open the `VERSION DETAILS` tab and try to find your model artifact on Cloud Storage.\n",
    "\n",
    "Let's deploy the model to an endpoint for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6e9ab8-e08b-453f-85be-59b95c933b73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/888342260584/locations/us-central1/endpoints/1390056475904180224/operations/6436079171037822976\n",
      "Endpoint created. Resource name: projects/888342260584/locations/us-central1/endpoints/1390056475904180224\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/888342260584/locations/us-central1/endpoints/1390056475904180224')\n"
     ]
    }
   ],
   "source": [
    "endpoint_upload = aiplatform.Endpoint.create(display_name=\"bootkon-endpoint-upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d151a-c23e-4da0-923c-7bb3cc807972",
   "metadata": {},
   "source": [
    "The next code chunk will take around 10min. We don't want to wait for that, so we set `sync=False` and look at the result later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f9482d-125c-43fd-a5fb-2042cd6f4b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/888342260584/locations/us-central1/endpoints/1390056475904180224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f11784d58a0> \n",
       "resource name: projects/888342260584/locations/us-central1/endpoints/1390056475904180224"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy Endpoint model backing LRO: projects/888342260584/locations/us-central1/endpoints/1390056475904180224/operations/7148773812069203968\n",
      "Deploying model to Endpoint : projects/888342260584/locations/us-central1/endpoints/3718136008278016000\n",
      "Deploy Endpoint model backing LRO: projects/888342260584/locations/us-central1/endpoints/3718136008278016000/operations/1055403516236922880\n",
      "Endpoint model deployed. Resource name: projects/888342260584/locations/us-central1/endpoints/1390056475904180224\n"
     ]
    }
   ],
   "source": [
    "vertex_model_upload.deploy(\n",
    "    deployed_model_display_name=\"bootkon-model-upload\",\n",
    "    endpoint=endpoint_upload,\n",
    "    machine_type=\"n2-standard-2\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581c19-50bd-4fc2-be82-9bc564c4baca",
   "metadata": {},
   "source": [
    "The next chunk lists the currently deployed models. While the model is deploying, it wont's show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d9d2a32-7847-47a1-aa44-ed9f41f538b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_upload.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2828f-8fc4-4f95-bd02-04995afd9aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Train and serve model using custom containers\n",
    "\n",
    "In this section, we will train a `RandomForestClassifier` using **custom containers** on Vertex AI and deploy it for real-time predictions. Instead of using pre-built containers, we will package our training and prediction logic into Docker containers, allowing for **full control over dependencies, runtime environments, and scalability**. \n",
    "\n",
    "The process consists of two main steps:\n",
    "1. **Model Training:** We will preprocess the dataset, train a model and save it as a serialized `joblib` file. The trained model will be uploaded to Cloud Storage for deployment.\n",
    "2. **Model Serving:** Using a separate container, the stored model will be loaded from Cloud Storage, and an API will be exposed via Flask (or **FastAPI** in production) to handle inference requests.\n",
    "\n",
    "By leveraging Vertex AI‚Äôs custom training and prediction services, we can achieve a **scalable, managed ML workflow** while keeping complete flexibility over the training and deployment pipeline.\n",
    "\n",
    "We will create the following files:\n",
    "\n",
    "- `train/Dockerfile`: Dockerfile for the training container\n",
    "- `train/train.py`: Training script\n",
    "- `predict/Dockerfile`: Dockerfile for the prediction container\n",
    "- `predict/predict.py`: Prediction script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035fe4c-2bf7-4f77-b0a1-1b9403db3f1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we configure docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a1764e-48d6-42ef-a4b1-30fd2d56fc87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739520189.216623 3264411 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155ee55-2913-47dc-9343-636bee903712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa8927f0-3c70-4d10-a353-4f223ffb5f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkdir -p train predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb918e4-b734-4975-9f6a-bde944794b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY train.py /app/train.py\n",
    "\n",
    "RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage fsspec gcsfs\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/app/train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa30621-9df7-43ab-abcd-64f6c5b6f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict/Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY predict.py /app/predict.py\n",
    "\n",
    "RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage google-cloud-aiplatform fsspec gcsfs flask\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"/app/predict.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aabe48-5c59-48f7-afed-0b7c4c40eea7",
   "metadata": {},
   "source": [
    "The `train.py` script trains a `RandomForestClassifier` using scikit-learn, saves it as a `joblib` file, and uploads it to Cloud Storage. It reads the training data (`X_train` and `y_train`) from CSV files provided as command-line arguments and retrieves the target storage directory from the `AIP_MODEL_DIR` environment variable. The trained model is stored in GCS for later deployment on Vertex AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af82d431-2684-4364-a0fe-01610e9ea191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from google.cloud import storage\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "X_train = pd.read_csv(sys.argv[1])\n",
    "y_train = pd.read_csv(sys.argv[2])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=8, verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, \"model.joblib\")\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(AIP_MODEL_DIR.split(\"/\")[2])\n",
    "blob = bucket.blob(\"/\".join(AIP_MODEL_DIR.split(\"/\")[3:]) + \"/model.joblib\")\n",
    "blob.upload_from_filename(\"model.joblib\")\n",
    "print(f\"Wrote model to {AIP_MODEL_DIR}/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5d45d-be53-405c-994b-b9367f6793a9",
   "metadata": {},
   "source": [
    "The `predict.py` script is a flask-based prediction server designed for deployment on Vertex AI using custom containers. It retrieves the model artifacts from Cloud Storage using `prediction_utils.download_model_artifacts()`, loads the model with `joblib`, and exposes two API endpoints:\n",
    "\n",
    "- **`/predict`** for inference  \n",
    "- **`/health`** for monitoring the service status  \n",
    "\n",
    "The script reads environment variables such as `AIP_STORAGE_URI` for downloading the model and `AIP_PREDICT_ROUTE` for defining the prediction route dynamically. \n",
    "\n",
    "‚ö† **In production,** it is recommended to use **FastAPI** instead of Flask due to its superior performance, asynchronous capabilities, and built-in request validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63cccaed-feed-4e7d-a653-5b2ec5501aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict/predict.py\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import flask\n",
    "import numpy as np\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "AIP_STORAGE_URI = os.environ[\"AIP_STORAGE_URI\"]\n",
    "print(f\"Downloading model from {AIP_STORAGE_URI}/model.joblib\")\n",
    "prediction_utils.download_model_artifacts(AIP_STORAGE_URI)\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "@app.route(os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\"), methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = flask.request.get_json()\n",
    "    inputs = np.array(data[\"instances\"])\n",
    "    predictions = model.predict(inputs).tolist()\n",
    "    return flask.jsonify({\"predictions\": predictions})\n",
    "\n",
    "@app.route(os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\"), methods=[\"GET\"])\n",
    "def health_check():\n",
    "    print(\"Received health check\")\n",
    "    return flask.jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=int(os.environ.get(\"AIP_HTTP_PORT\", 8080)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba306444-44bb-464e-b554-c043e45ea557",
   "metadata": {},
   "source": [
    "We will store the container images in a docker repository named `bootkon`. Let's create it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55c6a141-8473-4483-809f-56ea486a02d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [bootkon]\n",
      "Waiting for operation [projects/astute-ace-336608/locations/us-central1/operati\n",
      "ons/a240e7fd-936b-403d-9082-8d4f21e4b8cd] to complete...done.                  \n",
      "Created repository [bootkon].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create bootkon --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39408f-a804-4b0d-9ec7-9d3399d9fce9",
   "metadata": {},
   "source": [
    "We can use Cloud Build to build to image and automatically push it to the container repository we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b3cefa6-5d11-46be-9e6a-9d4209afb46b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 881 bytes before compression.\n",
      "Uploading tarball of [.] to [gs://astute-ace-336608_cloudbuild/source/1739520193.925865-5561e710d4574e238008430771be4824.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/astute-ace-336608/locations/us-central1/builds/4a4cfab3-abc7-475a-94d1-6ee2b331639d].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/4a4cfab3-abc7-475a-94d1-6ee2b331639d?project=888342260584 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4a4cfab3-abc7-475a-94d1-6ee2b331639d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://astute-ace-336608_cloudbuild/source/1739520193.925865-5561e710d4574e238008430771be4824.tgz#1739520194311893\n",
      "Copying gs://astute-ace-336608_cloudbuild/source/1739520193.925865-5561e710d4574e238008430771be4824.tgz#1739520194311893...\n",
      "/ [1 files][  710.0 B/  710.0 B]                                                \n",
      "Operation completed over 1 objects/710.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/5 : FROM python:3.10-slim\n",
      "3.10-slim: Pulling from library/python\n",
      "c29f5b76f736: Already exists\n",
      "74e68b11a1c1: Pulling fs layer\n",
      "a477a912afa7: Pulling fs layer\n",
      "8c67a072a8ad: Pulling fs layer\n",
      "8c67a072a8ad: Verifying Checksum\n",
      "8c67a072a8ad: Download complete\n",
      "74e68b11a1c1: Verifying Checksum\n",
      "74e68b11a1c1: Download complete\n",
      "a477a912afa7: Verifying Checksum\n",
      "a477a912afa7: Download complete\n",
      "74e68b11a1c1: Pull complete\n",
      "a477a912afa7: Pull complete\n",
      "8c67a072a8ad: Pull complete\n",
      "Digest: sha256:66aad90b231f011cb80e1966e03526a7175f0586724981969b23903abac19081\n",
      "Status: Downloaded newer image for python:3.10-slim\n",
      " ---> b791f5ccaef8\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Running in a4c9a35c5231\n",
      "Removing intermediate container a4c9a35c5231\n",
      " ---> 4d95c7cd4069\n",
      "Step 3/5 : COPY train.py /app/train.py\n",
      " ---> c2646dbfcd40\n",
      "Step 4/5 : RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage fsspec gcsfs\n",
      " ---> Running in cca02c4961ea\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container cca02c4961ea\n",
      " ---> e4ffbc86d970\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/app/train.py\"]\n",
      " ---> Running in 4516e8570b5c\n",
      "Removing intermediate container 4516e8570b5c\n",
      " ---> 07a4e7f38388\n",
      "Successfully built 07a4e7f38388\n",
      "Successfully tagged us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train]\n",
      "3261d1bfd94d: Preparing\n",
      "11e08faae4bc: Preparing\n",
      "8ee980b23ac5: Preparing\n",
      "cd9604f50b89: Preparing\n",
      "f52fdf687483: Preparing\n",
      "93d4d0473476: Preparing\n",
      "7914c8f600f5: Preparing\n",
      "93d4d0473476: Waiting\n",
      "7914c8f600f5: Waiting\n",
      "11e08faae4bc: Pushed\n",
      "cd9604f50b89: Pushed\n",
      "8ee980b23ac5: Pushed\n",
      "f52fdf687483: Pushed\n",
      "93d4d0473476: Pushed\n",
      "7914c8f600f5: Pushed\n",
      "3261d1bfd94d: Pushed\n",
      "latest: digest: sha256:3df3f4b2115f482e5e03d32a80728ae40c2ac79db10c857b916244588551a2f8 size: 1786\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                                        STATUS\n",
      "4a4cfab3-abc7-475a-94d1-6ee2b331639d  2025-02-14T08:03:14+00:00  1M18S     gs://astute-ace-336608_cloudbuild/source/1739520193.925865-5561e710d4574e238008430771be4824.tgz  us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd train && gcloud builds submit --region={REGION} --tag={TRAIN_IMAGE_URI} --timeout=1h --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff5982-15a5-4dcb-9685-c669a22381dc",
   "metadata": {},
   "source": [
    "Let's do the same thing for the prediction image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "515e0f7c-43ff-454f-83b8-7c353b374a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 3 file(s) totalling 1.2 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://astute-ace-336608_cloudbuild/source/1739520277.405046-dfbbb67ff0df42cd8d6960c99c151e4b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/astute-ace-336608/locations/us-central1/builds/b250065d-2ef6-405c-9e2d-9fcaf541c0ba].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/b250065d-2ef6-405c-9e2d-9fcaf541c0ba?project=888342260584 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"b250065d-2ef6-405c-9e2d-9fcaf541c0ba\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://astute-ace-336608_cloudbuild/source/1739520277.405046-dfbbb67ff0df42cd8d6960c99c151e4b.tgz#1739520277719673\n",
      "Copying gs://astute-ace-336608_cloudbuild/source/1739520277.405046-dfbbb67ff0df42cd8d6960c99c151e4b.tgz#1739520277719673...\n",
      "/ [1 files][  961.0 B/  961.0 B]                                                \n",
      "Operation completed over 1 objects/961.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  4.608kB\n",
      "Step 1/6 : FROM python:3.10-slim\n",
      "3.10-slim: Pulling from library/python\n",
      "c29f5b76f736: Already exists\n",
      "74e68b11a1c1: Pulling fs layer\n",
      "a477a912afa7: Pulling fs layer\n",
      "8c67a072a8ad: Pulling fs layer\n",
      "8c67a072a8ad: Verifying Checksum\n",
      "8c67a072a8ad: Download complete\n",
      "74e68b11a1c1: Verifying Checksum\n",
      "74e68b11a1c1: Download complete\n",
      "a477a912afa7: Verifying Checksum\n",
      "a477a912afa7: Download complete\n",
      "74e68b11a1c1: Pull complete\n",
      "a477a912afa7: Pull complete\n",
      "8c67a072a8ad: Pull complete\n",
      "Digest: sha256:66aad90b231f011cb80e1966e03526a7175f0586724981969b23903abac19081\n",
      "Status: Downloaded newer image for python:3.10-slim\n",
      " ---> b791f5ccaef8\n",
      "Step 2/6 : WORKDIR /app\n",
      " ---> Running in cb91fdd927c1\n",
      "Removing intermediate container cb91fdd927c1\n",
      " ---> b7f5879a1ebc\n",
      "Step 3/6 : COPY predict.py /app/predict.py\n",
      " ---> ce7c2e81719d\n",
      "Step 4/6 : RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage google-cloud-aiplatform fsspec gcsfs flask\n",
      " ---> Running in 8445bb00e041\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 8445bb00e041\n",
      " ---> 07f2e75e5b95\n",
      "Step 5/6 : EXPOSE 8080\n",
      " ---> Running in 3cf586da5684\n",
      "Removing intermediate container 3cf586da5684\n",
      " ---> 116648f185f4\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"/app/predict.py\"]\n",
      " ---> Running in 7a469addbf34\n",
      "Removing intermediate container 7a469addbf34\n",
      " ---> b99e0610cc6b\n",
      "Successfully built b99e0610cc6b\n",
      "Successfully tagged us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict]\n",
      "306df139da4b: Preparing\n",
      "2f077ff0d52d: Preparing\n",
      "2ce059fc280e: Preparing\n",
      "cd9604f50b89: Preparing\n",
      "f52fdf687483: Preparing\n",
      "93d4d0473476: Preparing\n",
      "7914c8f600f5: Preparing\n",
      "93d4d0473476: Waiting\n",
      "7914c8f600f5: Waiting\n",
      "f52fdf687483: Layer already exists\n",
      "cd9604f50b89: Layer already exists\n",
      "93d4d0473476: Layer already exists\n",
      "2f077ff0d52d: Pushed\n",
      "2ce059fc280e: Pushed\n",
      "7914c8f600f5: Pushed\n",
      "306df139da4b: Pushed\n",
      "latest: digest: sha256:be0a3754390ac788b7feaf9ff47427636dd65183b5e5ad8690d4ba8a7e731f25 size: 1786\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                                          STATUS\n",
      "b250065d-2ef6-405c-9e2d-9fcaf541c0ba  2025-02-14T08:04:37+00:00  1M37S     gs://astute-ace-336608_cloudbuild/source/1739520277.405046-dfbbb67ff0df42cd8d6960c99c151e4b.tgz  us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd predict && gcloud builds submit --region={REGION} --tag={PREDICT_IMAGE_URI} --timeout=1h --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5ae53-f124-419a-b4a9-f6ca41b0fac7",
   "metadata": {},
   "source": [
    "Now that the container is ready, we can run it as `CustomContainerTrainingJob` -- giving the training data set as arguments. This will take around 5-10min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15a6594f-c9ed-40cc-a919-c7a92114ee4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name = \"bootkon-custom\",\n",
    "    container_uri = TRAIN_IMAGE_URI,\n",
    "    model_serving_container_image_uri = PREDICT_IMAGE_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c03b9420-e8ec-4b13-9f9e-9ae24a71a81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://astute-ace-336608-bucket/aiplatform-custom-training-2025-02-14-08:06:16.843 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2976178930425266176?project=888342260584\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3548195456729219072?project=888342260584\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/888342260584/locations/us-central1/trainingPipelines/2976178930425266176\n",
      "Model available at projects/888342260584/locations/us-central1/models/2302690709376860160\n"
     ]
    }
   ],
   "source": [
    "vertex_model_custom = job.run(\n",
    "    args=[\n",
    "        f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\",\n",
    "        f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7315374-cd73-4bd8-8e2c-20815afa917a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/888342260584/locations/us-central1/endpoints/3718136008278016000/operations/5526633521285693440\n",
      "Endpoint created. Resource name: projects/888342260584/locations/us-central1/endpoints/3718136008278016000\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/888342260584/locations/us-central1/endpoints/3718136008278016000')\n"
     ]
    }
   ],
   "source": [
    "endpoint_custom = aiplatform.Endpoint.create(display_name=\"bootkon-endpoint-custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a58fd6-c01f-45e7-b6ab-13a800f72896",
   "metadata": {},
   "source": [
    "We also deploy this model and don't wait for it to finish (`sync=False`) -- instead we come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02219939-8414-4bde-94ad-d3b97b003077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f1178499a50> \n",
       "resource name: projects/888342260584/locations/us-central1/endpoints/3718136008278016000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_model_custom.deploy(\n",
    "    deployed_model_display_name=\"bootkon-model-custom\",\n",
    "    endpoint=endpoint_custom,\n",
    "    machine_type=\"n2-standard-2\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770835f3-1772-472f-8cf3-9a777290e3dc",
   "metadata": {},
   "source": [
    "## Step 5: Train and deploy models using Vertex Pipelines\n",
    "\n",
    "In this section, we will train a `RandomForestClassifier` using Vertex AI Pipelines and Kubeflow Pipelines (KFP) and deploy it for real-time predictions. Unlike the custom container approach, we will define a pipeline-based workflow for automated training, model storage, and deployment. This approach allows us to achieve repeatability, scalability, and automation for end-to-end ML workflows on Google Cloud.\n",
    "\n",
    "By leveraging Vertex AI Pipelines, we can create a fully managed, automated ML pipeline that integrates seamlessly with GCP services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc9e9c47-a872-4523-988a-4a36185daae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl, compiler\n",
    "\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.dsl import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb54bb-3b3c-4d2b-9ab5-cc2de258b502",
   "metadata": {},
   "source": [
    "Next, we create a Kubeflow pipeline that automates the training, model upload, and deployment process in Vertex AI.\n",
    "\n",
    "**Pipeline Steps**\n",
    "\n",
    "1. Define a Unique Model Directory:\n",
    "* The pipeline assigns a unique Cloud Storage path for storing the trained model using `PIPELINE_JOB_ID_PLACEHOLDER`, ensuring each run has an isolated model directory.\n",
    "\n",
    "2. Run a Custom Training Job\n",
    "* Uses `CustomTrainingJobOp` to launch a training job on Vertex AI.\n",
    "* The training script is executed inside a custom container (`TRAIN_IMAGE_URI`).\n",
    "* The trained model is stored in the dynamically created directory (`AIP_MODEL_DIR`).\n",
    "\n",
    "3. Import the Trained Model as an Artifact\n",
    "* The `importer_node.importer` step converts the saved model directory into an `UnmanagedContainerModel`, allowing it to be used by Vertex AI.\n",
    "\n",
    "4. Upload the Model to Vertex AI\n",
    "* The `ModelUploadOp` registers the trained model in Vertex AI, making it available for deployment.\n",
    "\n",
    "5. Create an Endpoint for Deployment\n",
    "* `EndpointCreateOp` initializes a new prediction endpoint in Vertex AI.\n",
    "\n",
    "6. Deploy the Model to the Endpoint\n",
    "* `ModelDeployOp` deploys the registered model to the created endpoint with a dedicated `n1-standard-4` machine.\n",
    "\n",
    "**Key Features**\n",
    "- **Dynamically generated model path** ensures each pipeline run has an isolated model storage.\n",
    "- **Custom container training** allows full control over the training process.\n",
    "- **Automated model registration and deployment** simplifies the end-to-end MLOps workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2797c69-cc93-40c1-89b9-af39003780f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"bootkon-pipeline\")\n",
    "def pipeline(\n",
    "    X_train: str,\n",
    "    y_train: str,\n",
    "    project: str = PROJECT_ID\n",
    "):\n",
    "    model_dir = f\"{PIPELINE_ROOT}/model-{kfp.dsl.PIPELINE_JOB_ID_PLACEHOLDER}\"\n",
    "    custom_job_task = CustomTrainingJobOp(\n",
    "        project=project,\n",
    "        display_name=\"bootkon-model-pipeline\",\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"containerSpec\": {\n",
    "                    \"args\": [X_train, y_train],\n",
    "                    \"env\": [{\"name\": \"AIP_MODEL_DIR\", \"value\": model_dir}],\n",
    "                    \"imageUri\": TRAIN_IMAGE_URI,\n",
    "                },\n",
    "                \"replicaCount\": \"1\",\n",
    "                \"machineSpec\": {\n",
    "                    \"machineType\": \"n1-standard-4\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=model_dir,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": PREDICT_IMAGE_URI,\n",
    "            },\n",
    "        },\n",
    "    ).after(custom_job_task)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=\"bootkon-pipeline-model\",\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"bootkon-endpoint-pipeline\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"bootkon-pipeline-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b15037-7884-4993-91f0-a784cb93e60c",
   "metadata": {},
   "source": [
    "The following command compiles the `bootkon-pipeline` into a JSON file that can be submitted to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2db3cd1-8206-41cd-856a-3754b4d4c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"bootkon_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ef5ff-6ee9-41c0-bca7-b10b67a39ab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "And we submit it. Feel free to investigate the pipeline using the link that is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30b0b5e4-bf20-45d1-a9d1-35c39e1f563c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bootkon-pipeline-20250214081430?project=888342260584\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250214081430 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"bootkon-pipeline\",\n",
    "    template_path=\"bootkon_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    project=PROJECT_ID,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"X_train\": f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\",\n",
    "        \"y_train\": f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\"\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb4972-ec2f-4673-b19b-28915a1a79db",
   "metadata": {},
   "source": [
    "## Step 6: Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a56ffe-13f5-499c-a5ac-9e888f9d9239",
   "metadata": {},
   "source": [
    "We now should have several endpoints deployed. Let's check the endpoint from Step 3.2 (the ***upload*** model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "264ffe18-281c-4c2b-92a4-e74d447a0918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_upload.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cec05-ee1b-4566-89c0-5444ccaab4df",
   "metadata": {},
   "source": [
    "Let's make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3b95b55-5865-4049-bb5a-f21ea1b6d3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint_upload.predict(instances=X_test.head(4000).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5feec-6866-4f96-b98e-d9d3b498f80d",
   "metadata": {},
   "source": [
    "Most of them are ***not fraud*** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dd3fc97-fc43-4eba-a819-337d23f3d942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145ffaa-8e0f-459c-93e3-dd637a308868",
   "metadata": {},
   "source": [
    "But there are also a few fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ef453a4-2254-4f0d-8fde-0568483ff063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320aaf99-c110-4e05-ae1a-f23a8b214b61",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can do the same thing with the other endpoint we created through custom containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d08f56b-8e1b-43e0-85fb-65c03c7eb40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint_custom.predict(instances=X_test.head(4000).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "737ca3bd-5e91-4b66-a6c2-c5007f951a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77729c4e-516d-4cc4-9226-b90660df3e3c",
   "metadata": {},
   "source": [
    "## Investigate results in the Cloud Console\n",
    "\n",
    "<font color=\"red\"><b>Great job deploying all these models. Now, please go back to the lab in Cloud Shell and continue from there!</b></font>\n",
    "\n",
    "<img src=\"../docs/img/lab4/cloud_shell_4.png\" width=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6b093-aa93-4568-8b76-b734a70f3671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
