<!-- DO NOT EDIT .TUTORIAL.md BY HAND -- EDIT docs/TUTORIAL.md instead -->\n\n\n
<walkthrough-metadata>
  <meta name="title" content="Data & AI Boot-Kon" />
  <meta name="description" content="These labs include detailed step-by-step instructions to guide you. In addition to the labs, you’ll face several challenges that you’ll need to solve on your own or with your group. Groups will be assigned by the event organizers at the start of the event." />
  <meta name="keywords" content="data, ai, bigquery, vertexai, genai, notebook" />
  <meta name="component_id" content="12345" />
</walkthrough-metadata>


# Data and AI Boot-Kon

## Introduction

Welcome to bootkon. 

An imaginary company named FraudFix Technologies specializes in enhancing financial transaction security for Google Cloud enterprise customers across industries like finance and e-commerce.

**Your role**: As a senior data analytics/AI engineer at FraudFix Technologies, you will tackle the challenges of making financial transactions safer using machine learning. Your work will involve analyzing vast amounts of transaction data to detect and prevent fraud, as well as assessing customer sentiment regarding the quality of transaction services. You will leverage a unique synthetic dataset, which includes auto-generated data by Google Gemini and a public European credit card transaction dataset that has been PCA transformed and anonymized. This dataset will be used to train your models, reflecting real-world applications of GCP Data & AI in enhancing financial safety.

### Working with labs

You can insert commands into the terminal using the following icon on top of each code junk:
<walkthrough-cloud-shell-icon></walkthrough-cloud-shell-icon>

Please press this icon in the following code chunk:

```bash
echo "I'm ready to get started."
```

And execute by pressing the return key in the terminal that has been opened in the lower part of your screen.

Press the `START` button below to get started!

## Lab 1: Environment Setup

<walkthrough-tutorial-duration duration="30"></walkthrough-tutorial-duration>
<walkthrough-tutorial-difficulty difficulty="1"></walkthrough-tutorial-difficulty>
<bootkon-cloud-shell-note/>

In this lab you will grant permissions and set up a default VPC network as a preparatory step.

### **Choice of GCP Product and Service Location**

You are free to choose any GCP region location for your labs. Ensure all your resources are created in the chosen location to avoid connectivity issues and minimize latency and cost. If you don’t have a preferred GCP location, use ***us-central1*** for simplicity.

### **Setup your environment**

Open `vars.sh` <walkthrough-editor-open-file filePath="vars.sh"> in the Cloud Shell editor </walkthrough-editor-open-file> and adapt it. Don't forget to save it.

Now, export the variables to your environment:
```bash
source vars.sh
```

Verify that they have been set correctly:
```bash
echo "PROJECT_ID=$PROJECT_ID REGION=$REGION GCP_USERNAME=$GCP_USERNAME"
```

Please also select your project in the next widget and ignore the comment about creating a new project.

<walkthrough-project-setup></walkthrough-project-setup>

Have a look at <walkthrough-editor-open-file filePath="bootstrap.sh">`bootstrap.sh`</walkthrough-editor-open-file> and what it does; exeucte it:
```bash
./bootstrap.sh
```

Well done, your environment is now ready for the first lab!


## Lab 2: Data Ingestion

<walkthrough-tutorial-duration duration="45"></walkthrough-tutorial-duration>
<walkthrough-tutorial-difficulty difficulty="4"></walkthrough-tutorial-difficulty>
<bootkon-cloud-shell-note/>

Original document: [here](https://docs.google.com/document/d/1NAcQb9qUZsyGSe2yPQWKrBz18ZRVCL7X9e-NDs5lQbk/edit?usp=drive_link)


During this lab, you ingest fraudulent and non fraudulent transactions dataset into BigQuery using three methods:
* Method 1: Using BigLake with data stored in Google Cloud Storage (GCS)
* Method 2: Near real-time ingestion into BigQuery using [Cloud PubSub](https://cloud.google.com/pubsub)
* Method 3: Batch Ingestion into BigQuery using Dataproc Serverless

For all methods, we are ingesting data from the Google Cloud bucket you have created in the previous lab through `bootstrap.sh`. Feel free to have a look at the contents of this bucket:

### Method 1: External table using BigLake

BigLake tables let you query structured data in external data stores with access delegation. Access delegation decouples access to the BigLake table from access to the underlying data store. An external connection associated with a service account is used to connect to the data store.

Because the service account handles retrieving data from the data store, you only have to grant users access to the BigLake table. This lets you enforce fine-grained security at the table level, including row-level and column-level security. For BigLake tables based on Cloud Storage, you can also use dynamic data masking. To learn more about multi-cloud analytic solutions using BigLake tables with Amazon S3 or Blob Storage data, see BigQuery Omni.

Note that this section could also be done in the Google Cloud Console (the GUI). However, in this lab, we will do it on the command line.

First, we create the connection:
```bash
bq mk --connection --location=$REGION --project_id=$PROJECT_ID \
    --connection_type=CLOUD_RESOURCE fraud-transactions-conn
```

When you create a connection resource, BigQuery creates a unique system service account and associates it with the connection.
```bash
bq show --connection ${PROJECT_ID}.${REGION}.fraud-transactions-conn
```
Note the `serviceAccountID`. It should resemble `connection-...@...gserviceaccount.com`.

To connect to Cloud Storage, you must give the new connection read-only access to Cloud Storage so that BigQuery can access files on behalf of users. Let's assign the service account to a variable:
```bash
CONN_SERVICE_ACCOUNT=$(bq --format=prettyjson show --connection ${PROJECT_ID}.${REGION}.fraud-transactions-conn | jq -r ".cloudResource.serviceAccountId")
echo $CONN_SERVICE_ACCOUNT
```

And grant it access to Cloud Storage:
```bash
gcloud storage buckets add-iam-policy-binding gs://${PROJECT_ID}-bucket \
--role=roles/storage.objectViewer \
--member=serviceAccount:$CONN_SERVICE_ACCOUNT
```

Next, we create a dataset that our external table will live in:
```bash
bq --location=${REGION} mk -d ml_datasets
```

Go to the [BigQuery Console](https://console.cloud.google.com/bigquery). check that the dataset has been created successfully (Note: you may need to click "refresh contents" from the 3-dot menu for the project in the Explorer).

Finally, create a table in BigQuery pointing to the data in Cloud Storage:

```bash
bq mk --table \
  --external_table_definition=@PARQUET="gs://${PROJECT_ID}-bucket/bootkon-data/parquet/ulb_fraud_detection/*"@projects/${PROJECT_ID}/locations/${REGION}/connections/fraud-transactions-conn \
  ml_datasets.ulb_fraud_detection_biglake
```

Go to the [BigQuery Console](https://console.cloud.google.com/bigquery) console again and open the dataset and table you just created. Click on `Query` and insert the following SQL query.

```sql
SELECT * FROM `<walkthrough-project-id/>.ml_datasets.ulb_fraud_detection_biglake` LIMIT 1000;
```

Note that you can also execute a query using the `bq` tool:

```bash
bq --location=$REGION query --nouse_legacy_sql "SELECT Time, V1, Amount, Class FROM <walkthrough-project-id/>.ml_datasets.ulb_fraud_detection_biglake LIMIT 10;"
```

Note that the data we are querying still resides on Cloud Storage and there are no copies stored in BigQuery. Using BigLake, BigQuery acts as query engine but not as storage layer.

### Method 2: Real time data ingestion into BigQuery using Pub/Sub

This variant of data ingestion allows real-time streaming into BigQuery using Pub/Sub.

We create an empty table and then stream data into it. For this to work, we need to specify a schema. Have a look at <walkthrough-editor-open-file filePath="src/data_ingestion/fraud_detection_bigquery_schema.json">`fraud_detection_bigquery_schema.json`</walkthrough-editor-open-file>. This is the schema we are going to use.

Create an empty table using this schema:
```bash
bq --location=$REGION mk --table \
<walkthrough-project-id/>:ml_datasets.ulb_fraud_detection_pubsub src/data_ingestion/fraud_detection_bigquery_schema.json
```

We also need to a Pub/Sub schema:
```bash
gcloud pubsub schemas create fraud-detection-schema \
    --project=$PROJECT_ID  \
    --type=AVRO \
    --definition-file=src/data_ingestion/fraud_detection_pubsub_schema.json
```

And them create a Pub/Sub topic using this schema:
```bash
gcloud pubsub topics create fraud-detection-topic \
    --project=$PROJECT_ID  \
    --schema=fraud-detection-schema \
    --message-encoding=BINARY
```

We also need to give Pub/Sub permissions to write data to BigQuery. The Pub/Sub service account is comprised of the project number (not the id) and an identifier. Let's first figure out the number:
```bash
export PROJECT_NUM=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
export PUBSUB_SERVICE_ACCOUNT="service-${PROJECT_NUM}@gcp-sa-pubsub.iam.gserviceaccount.com"
echo $PUBSUB_SERVICE_ACCOUNT
```

And grant the service account access to BigQuery:

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member=serviceAccount:$PUBSUB_SERVICE_ACCOUNT --role=roles/bigquery.dataEditor

gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member=serviceAccount:$PUBSUB_SERVICE_ACCOUNT --role=roles/bigquery.jobUser  
```

Next, we create the Pub/Sub subscription:
```bash
gcloud pubsub subscriptions create fraud_detection-subscription \
    --project=$PROJECT_ID  \
    --topic=fraud_detection-topic \
    --bigquery-table=$PROJECT_ID.ml_datasets.ulb_fraud_detection_pubsub \
    --use-topic-schema  
```

Feel free to [check it out in the Pub/Sub console](https://console.cloud.google.com/cloudpubsub/subscription).

Since we'll be using Python, let's install the Python <walkthrough-editor-open-file filePath="requirements.txt">packages</walkthrough-editor-open-file> we want to make use of:
```bash
pip install -r requirements.txt
```

Please have a look at <walkthrough-editor-open-file filePath="src/data_ingestion/import_csv_to_bigquery_1.py">`import_csv_to_bigquery_1.py`</walkthrough-editor-open-file>. This script loads CSV files from Cloud Storage, parses it in Python, and sends it to Pub/Sub - row by row.

Let's execute it.
```bash
./src/data_ingestion/import_csv_to_bigquery_1.py
```

Each line you see on the screen corresponds to one transaction being send to Pub/Sub and written to BigQuery. It would take approximately 40 to 60 minutes for it to finish. So, please cancel the command using 'CTRL + C'.

<!-- 
We can make this faster by using different parameters for Pub/Sub. First, remove all rows you just ingested:
```bash
bq --location=$REGION query --nouse_legacy_sql "DELETE FROM <walkthrough-project-id/>.ml_datasets.ulb_fraud_detection_pubsub WHERE true;"
```

Next, have a look at <walkthrough-editor-open-file filePath="src/data_ingestion/import_csv_to_bigquery_2.py">import_csv_to_bigquery_2.py</walkthrough-editor-open-file>. Can you make out the difference to the first script? Let's execute it:
```bash
./src/data_ingestion/import_csv_to_bigquery_2.py
```
-->

### Method 3: Ingestion using Cloud Dataproc (Apache Spark)

Google Cloud Dataproc is a fully managed and scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Dataproc allows data to be loaded and also transformed or pre-processed as it is brought in.

Create an empty BigQuery table:
```bash
bq --location=$REGION mk --table \
<walkthrough-project-id/>:ml_datasets.ulb_fraud_detection_dataproc src/data_ingestion/fraud_detection_bigquery_schema.json
```

Open <walkthrough-editor-select-line filePath="src/data_ingestion/import_parquet_to_bigquery.py" startLine="4" endLine="4" startCharacterOffset="14" endCharacterOffset="31">import_parquet_to_bigquery.py</walkthrough-editor-select-line> in the Cloud Shell editor and replace the project id with your project id. Don't forget to save.

Execute it:
```bash
gcloud dataproc batches submit pyspark src/data_ingestion/import_parquet_to_bigquery.py \
    --project=$PROJECT_ID \
    --region=$REGION \
    --deps-bucket=gs://${PROJECT_ID}-bucket
```

While the command is still running, open the [DataProc Console](https://console.cloud.google.com/dataproc/batches) and monitor the job.

After the Dataproc job completes, confirm that data has been loaded into the BigQuery table. You should see over 200,000 records, but the exact count isn't critical:
```bash
bq --location=$REGION query --nouse_legacy_sql "SELECT count(*) as count FROM <walkthrough-project-id/>.ml_datasets.ulb_fraud_detection_dataproc;"
```

You've nailed the data ingestion lab -- great job!

<walkthrough-conclusion-trophy></walkthrough-conclusion-trophy>

## Lab 3: Dataform

<walkthrough-tutorial-duration duration="45"></walkthrough-tutorial-duration>
<walkthrough-tutorial-difficulty difficulty="3"></walkthrough-tutorial-difficulty>
<bootkon-cloud-shell-note/>

Original document [here](https://docs.google.com/document/d/1NxfggQunrCn6ZfwGXAaA_lABDmXtRsfH88jkMDbqlJo/edit?usp=drive_link)

Goal of the lab: We gather user feedback to assess the impact of model adjustments on real-world use (prediction), ensuring that our fraud detection system effectively balances accuracy with user satisfaction. Use Dataform , BigQuery and Gemini to Perform sentiment analysis of customer feedback.

CAUTION:  
This lab is for educational purposes only and should be used with caution in production environments. Google Cloud Platform (GCP) products are changing frequently, and screenshots and instructions might become inaccurate over time. Always refer to the latest GCP documentation for the most up-to-date information.


### ***READING Section : Read the following Explanation of what dataform is and what is its purpose*** 

### **Dataform** 

Dataform is a fully managed service that helps data teams build, version control, and orchestrate SQL workflows in BigQuery. It provides an end-to-end experience for data transformation, including:

* Table definition: Dataform provides a central repository for managing table definitions, column descriptions, and data quality assertions. This makes it easy to keep track of your data schema and ensure that your data is consistent and reliable.  
* Dependency management: Dataform automatically manages the dependencies between your tables, ensuring that they are always processed in the correct order. This simplifies the development and maintenance of complex data pipelines.  
* Orchestration: Dataform orchestrates the execution of your SQL workflows, taking care of all the operational overhead. This frees you up to focus on developing and refining your data pipelines.

Dataform is built on top of Dataform Core, an open source SQL-based language for managing data transformations. Dataform Core provides a variety of features that make it easy to develop and maintain data pipelines, including:

* Incremental updates: Dataform Core can incrementally update your tables, only processing the data that has changed since the last update. This can significantly improve the performance and scalability of your data pipelines.  
* Slowly changing dimensions: Dataform Core provides built-in support for slowly changing dimensions, which are a common type of data in data warehouses. This simplifies the development and maintenance of data pipelines that involve slowly changing dimensions.  
* Reusable code: Dataform Core allows you to write reusable code in JavaScript, which can be used to implement complex data transformations and workflows.

Dataform is integrated with a variety of other Google Cloud services, including GitHub, GitLab, Cloud Composer, and Workflows. This makes it easy to integrate Dataform with your existing development and orchestration workflows.  
Benefits of using Dataform in Google Cloud  
There are many benefits to using Dataform in Google Cloud, including:

* Increased productivity: Dataform can help you to increase the productivity of your data team by automating the development, testing, and execution of data pipelines.  
* Improved data quality: Dataform can help you to improve the quality of your data by providing a central repository for managing table definitions, column descriptions, and data quality assertions.  
* Reduced costs: Dataform can help you to reduce the costs associated with data processing by optimizing the execution of your SQL workflows.  
* Increased scalability: Dataform can help you to scale your data pipelines to meet the needs of your growing business.

### **Use cases for Dataform**

Dataform can be used for a variety of use cases, including:

* Data warehousing: Dataform can be used to build and maintain data warehouses that are scalable and reliable.  
* Data engineering: Dataform can be used to develop and maintain data pipelines that transform and load data into data warehouses.  
* Data analytics: Dataform can be used to develop and maintain data pipelines that prepare data for analysis.  
* Machine learning: Dataform can be used to develop and maintain data pipelines that prepare data for machine learning models.

### ***LAB Section : Dataform Prerequisites*** 

**SKIP below Steps 1, 2 and 3 if you already completed LAB 1** 

1. ### **Enable Services API (you can skip this step if you completed LAB 1\)**

Ensure all necessary APIs (BigQuery API, Vertex AI API, BigQuery Connection API, Dataform API, Secret Manager API) are [enabled](https://console.cloud.google.com/flows/enableapi?apiid=storage-component.googleapis.com%2Cpubsub.googleapis.com%2Cbigquery.googleapis.com%2Cbigqueryconnection.googleapis.com%2Caiplatform.googleapis.com&_ga=2.132962701.243207769.1688884437-279425947.1688884437)

2. ### **Create a connection to an external data source in BigQuery (you can skip this step if you completed LAB 1\)**

* Create an External Connection (Enable BQ Connection API if not already done) and note down the Service Account id from the connection configuration details:  
* Click the \+ADD button on the BigQuery Explorer pane (in the left of the BigQuery console) and click “Connection to external data sources” in the popular sources listed  
* Select Connection type as “Vertex AI remote models , remote functions and Biglake” and provide “fraud-transactions-conn” as Connection ID, select Multi Region location type.  
  ![][image2]  
* Once the connection is created, take a note of the Service Account generated from the connection configuration details

3. ### **Grant Permissions  (you can skip this step if you completed LAB 1\)**

In this step we will grant permissions to the Service Account to access the Vertex AI service:  
Open IAM and add the Service Account you copied after creating the external connection as the Principal and select “Vertex AI User” Role.

4. ### **Using Large Language Models from Vertex AI  (info only)**

Google Cloud’s language models are available within the Vertex AI Studio inside the Vertex AI service.  
**![][image3]**

5. ### **Prompt design (info only)**

Prompt design is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model.  
If you need to understand this concept a bit more this is a page that introduces some basic concepts, strategies, and best practices to get you started in designing prompts ([https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-prompt-design](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-prompt-design)).  
The reference page above also goes into the more advanced settings you can see on the right hand side of the prompt box such as temperature, top K, top P etc.

### ***LAB Section : Creating a Dataform Pipeline***

First step in implementing a pipeline in Dataform is to set up a repository and a development environment. Detailed quickstart and instructions can be found [here](https://cloud.google.com/dataform/docs/quickstart-create-workflow).

Navigate to the BigQuery section in the Google Cloud Platform console, and then select Dataform.  
![][image4]

1. ### **Create a Repository in Dataform** 

Click the “+ CREATE REPOSITORY” button near the top of the page.  
![][image5]  
Use the following values when creating the repository:

- Repository ID: “hackathon-repository”  
- Region: (us-central1)  
- Service Account: (Default Dataform service account)  
  ![][image6]

And click “CREATE”

2. ### **Dataform Service Account** 

Take note and save somewhere the newly created service account for Dataform.  
Example: service-112412469323@gcp-sa-dataform.iam.gserviceaccount.com

![][image7]  
Click “GO TO REPOSITORIES”, and then click on the “hackathon-repository”, the new repository you just created.  
![][image8]

3. ### **Create and initialize a Dataform development workspace**

You should now be in the “DEVELOPMENT WORKSPACES” tab of the hackathon-repository page.

1. Click add **Create development workspace**.  
2. In the **Create development workspace** window, do the following:  
   1. In the **Workspace ID** field, enter “hackathon-\<YOURLASTNAME\>-workspace” (replace \<YOURLASTNAME\> with your name)  
   2. Click **Create**.  
3. The development workspace page appears.  
4. Click on the newly created development workspace   
5. Click **Initialize workspace**.

6. You will copy the dataform files from the following repository, in the next steps.   
   [https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform](https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform)   
7. *Edit  **workflow\_settings.yaml** file :*   
* *Replace defaultDataset value with **ml\_datasets ,***   
* make sure defaultProject value is ***your project id***   
  ***Note:*** Nevermind if you have a different dataform core version, just continue

  *![][image9]*  
* Click on Install Packages ***Only*** ***Once***. You should see a message at the bottom of the page:

  *Package installation succeeded*

8. *Remove the default auto-generated SQLX files; Delete the following files from the “definitions” folder:*  
* *first\_view.sqlx*  
* *second\_view.sqlx*

  *![][image10]*

9. *Click on definitions and create a new directory called “models”:* 

   *![][image11]*

10. *Click on models directory and create 2 new files ;  (make sure all file names are in lowercase and avoid adding spaces to the file names)*  
* [create\_dataset.sqlx](https://github.com/dace-de/bootkon-h2-2024/blob/main/dataform/definitions/models/create_dataset.sqlx)  
* [llm\_model\_connection.sqlx](https://github.com/dace-de/bootkon-h2-2024/blob/main/dataform/definitions/models/llm_model_connection.sqlx)

	  
Those files should be created under ***definitions/models*** directory

*Example:*

*![][image12]*

11. *Copy the contents from [https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform/definitions/models](https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform/definitions/models)  to each of those files.*  
12. *Click on definitions and create 3 new files: (make sure all file names are in lowercase and avoid adding spaces to the file names)*  
* [mview\_ulb\_fraud\_detection.sqlx](https://github.com/dace-de/bootkon-h2-2024/blob/main/dataform/definitions/mview_ulb_fraud_detection.sqlx)  
* [sentiment\_inference.sqlx](https://github.com/dace-de/bootkon-h2-2024/blob/main/dataform/definitions/sentiment_inference.sqlx)  
* [ulb\_fraud\_detection.sqlx](https://github.com/dace-de/bootkon-h2-2024/blob/main/dataform/definitions/ulb_fraud_detection.sqlx)


Those files should be created under ***definitions*** directory

*Example:* 

*![][image13]*

13. *Copy the contents from [https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform/definitions](https://github.com/dace-de/bootkon-h2-2024/tree/main/dataform/definitions) to each of those files.*  
14. *Set the database value to your project ID value in ulb\_fraud\_detection.sqlx file:*

    *![][image14]*

15. *In llm\_model\_connection.sqlx, replace the \`us.llm-connection\` connection with the connection name you have created in LAB 2 during the BigLake section.  If you have followed the steps in LAB 2, the connected name should be “**us.fraud-transactions-conn***”  
    Notice the usage of $ref in line 11, of ***definitions/mview\_ulb\_fraud\_detection.sqlx***  
     “sqlx” file. The advantages of using $ref in Dataform are

* Automatic Reference Management: Ensures correct fully-qualified names for tables and views, avoiding hardcoding and simplifying environment configuration.  
* Dependency Tracking: Builds a dependency graph, ensuring correct creation order and automatic updates when referenced tables change.  
* Enhanced Maintainability: Supports modular and reusable SQL scripts, making the codebase easier to maintain and less error-prone.

5. *Run the dataset creation by TAG. TAG allows you to just execute parts of the workflows and not the entire workflow. Click on Start Execution \> Tags \> "dataset\_ulb\_fraud\_detection\_llm” \> Start Execution*  
   

   *![][image15]*  
6. *Click on Details;*

   *![][image16]*

7. *Notice the Access Denied error on BigQuery for the dataform service account XXX@gcp-sa-dataform.iam.gserviceaccount.com;*

   *![][image17]*

8.  Go to IAM & Admin  \> Grant access and grant ***BigQuery Data Editor , BigQuery Job User and BigQuery Connection User***  to the data from the service account.  Click on Save.


   ![][image18]

   ***Note:*** If you encounter the following policy update screen, just click on update.

   ![][image19]

9. Go back to dataform from the BigQuery console, and retry step ***5***. Notice the execution status. It should be a success.  
   ![][image20]  
10. Click on Compiled graph and explore it;  
    Go to ***Dataform \> hackathon-\<lastname\>-workspace \> Compiled Graph***  
    ![][image21]

### ***LAB Section : Execute the workspace workflow***

1. For  the sentiment inference step to succeed . You need to grant the external connection service account the Vertex AI user privilege. More details can be found in this [link](https://cloud.google.com/bigquery/docs/generate-text-tutorial#grant-permissions). You can find the service account ID under BigQuery Studio \> Your project ID  (example: bootkon-dryrun24ber-886) \> External connections \> fraud-transactions-conn  
     
   ![][image22]  
    ![][image23]

2. Take note of the service account and grant it the ***Vertex AI User*** role.   
   ![][image24]  
     
3. *Back in your Dataform workspace, click **START EXECUTION** from the top menu, then* **“Execute actions”***.*  
   ![][image25]  
4. Click on ***ALL ACTIONS*** Tab then Click on ***START EXECUTION***  
   ![][image26]

5. Check the execution status. It should be a success.  
6. Verify the new table **sentiment\_inference** in the ml\_datasets dataset in BigQuery.  
7. Query the BigQuery table content (At this point you should be familiar with running BigQuery SQL)  
   

| *BigQuery SQL : Check few rows of* sentiment\_inference table |
| :---- |

```
SELECT distinct ml_generate_text_llm_result,
prompt,
Feedback
FROM `ml_datasets.sentiment_inference` LIMIT 10;
```
   

8. **\[Max 2 minutes\]** Discuss the table results within your team group.

9. Before moving to the challenge section of the Lab, go back to the CODE section of the Dataform workspace. At the top of the “Files” section on the left, click ***“Commit X Changes”*** (X should be about 7), add a commit message like, “Bootkon Lab 3”, then click “***Commit all files***” and then ***“Push to Default Branch”***   
   ***![][image27]***

You should now have the message   
***![][image28]***

# ***CHALLENGE Section : Production, Scheduling and Automation*** 

Automate and schedule the compilation and execution of the pipeline. This is done using release configurations and workflow configurations.

***Release Configurations:***  
Release configurations allow you to compile your pipeline code at specific intervals that suit your use case. You can define:

* Branch, Tag, or Commit SHA: Specify which version of your code to use.  
* Frequency: Set how often the compilation should occur, such as daily or weekly.  
* Compilation Overrides: Use settings for testing and development, such as running the pipeline in an isolated project or dataset/table.  
    
  Common practice includes setting up release configurations for both test and production environments. For more information, refer to the [release configuration documentation](https://cloud.google.com/dataform/docs/release-configurations).  
    
  **Workflow Configurations**  
    
  To execute a pipeline based on your specifications and code structure, you need to set up a workflow configuration. This acts as a scheduler where you define:  
    
* Release Configuration: Choose the release configuration to use.  
* Frequency: Set how often the pipeline should run.  
* Actions to Execute: Specify what actions to perform during each run.

  The pipeline will run at the defined frequency using the compiled code from the specified release configuration. For more information, refer to the [workflow configurations documentation](https://cloud.google.com/dataform/docs/workflow-configurations).

  *\[TASK\] Challenge : Take up to 10 minutes to Setup a Daily Frequency Execution of the Workflow*


  ***Goal:*** Set up a daily schedule to automate and execute the workflow you created.

1. Automate and schedule the pipeline’s compilation and execution.  
2. Define release configurations for one production environment (optionally: you can create another one for dev environment)  
3. Set up workflow configurations to schedule pipeline execution (use dataform service account).  
4. Set up a 3 minute frequency execution of the workflow you have created.  
     
     
   ***Note:*** If you are stuck and cannot figure out how to proceed after a few minutes, ask the event moderator for help.

**🥳🥳Congratulations on completing Lab 3\!**   
**You can now move on to Lab 4 for further practice. 🥳🥳**  




# **\<Lunch Time: 60 Minutes\>**

## Lab 4: ML Operations with Vertex AI

<walkthrough-tutorial-duration duration="60"></walkthrough-tutorial-duration>
<walkthrough-tutorial-difficulty difficulty="3"></walkthrough-tutorial-difficulty>
<bootkon-cloud-shell-note/>

Original document: [here](https://docs.google.com/document/d/1UdI1ffZdjy--_2xNmemQKzPCRXvCVw8JAroZqewiPMs/edit?usp=drive_link)


***Note: You can start Hands-on Lab 5 while the Hands-on Lab 4 training jobs in Notebooks 2 & 3 are still running.***  

**Finally, we create a Vertex AI Notebook (JupyterLab)**

1. Go to Vertex AI in the GCP console.

      ![alt vertexai](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/vertexai.png?raw=true)

      <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/vertexai.png?raw=true" target="_parent">View image</a>

2. Click on the Workbench section.

      ![alt workbench](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/workbench.png?raw=true)

      <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/workbench.png?raw=true" target="_parent">View image</a>

3. Select “User managed notebooks” 

      ![alt usermanagednotebooks](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/usermanagednotebooks.png?raw=true)

      <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/usermanagednotebooks.png?raw=true" target="_parent">View image</a>

4.  “Create new”

      ![alt createnew](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/createnew.png?raw=true)

      <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/createnew.png?raw=true" target="_parent">View image</a>

   

5. Name the notebook “***bootkon***” and leave the default network and environment. Leave the cheapest machine type; e2-standard-4 selected; 4 vCPUs and 16GB of RAM are more than enough to perform the ML labs using jupyter notebooks. Do not attach a GPU. Normally it takes around 10 minutes to get the instance created.

   ![alt notebookbootkon](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/notebookbootkon.png?raw=true)

   <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/notebookbootkon.png?raw=true" target="_parent">View image</a>

6. Open the Jupyter Lab;

   ![alt openjupyter](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/openjupyter.png?raw=true)

   <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/openjupyter.png?raw=true" target="_parent">View image</a>

7. From the Jupyter Lab top menu, click on Git \-\> Clone a Repository 

   ![alt clonerepo](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/clonerepo.png?raw=true)

   <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/clonerepo.png?raw=true" target="_parent">View image</a>

8. Enter [https://github.com/fhirschmann/bootkon-h2-2024.git](https://github.com/fhirschmann/bootkon-h2-2024.git) and click on **clone**

   ![alt clonerepo2](https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/clonerepo2.png?raw=true)

   <a href="https://github.com/fhirschmann/bootkon-ng/blob/main/img/lab1/clonerepo2.png?raw=true" target="_parent">View image</a>


## **\[Hands-on Lab \- 5\] Agent Builder and Gemini**

Original document [here](https://docs.google.com/document/d/1_8-HEEIKCCUkwoorpWq8lOI3M1Rn6HqY4SlCW8AitGg/edit?usp=drive_link)
